
== CP Subsystem

The CP subsystem is a component of a Hazelcast cluster that builds
an in-memory strongly consistent layer. It is accessed via
`HazelcastInstance.getCPSubsystem()`. Its data structures are _CP_ with
respect to the link:http://awoc.wolski.fi/dlib/big-data/Brewer_podc_keynote_2000.pdf[_CAP_ principle^],
i.e., they always maintain link:https://aphyr.com/posts/313-strong-consistency-models[linearizability^]
and prefer consistency over availability during network partitions.

Currently, the CP subsystem contains only the implementations
of Hazelcast's concurrency APIs. These APIs do not maintain large
states. For this reason, all members of a Hazelcast cluster do not
take part in the CP subsystem. The number of members that take part
in the CP subsystem is specified with `CPSubsystemConfig.setCPMemberCount(int)`.
Let's suppose the number of CP members is configured as **C**. Then, when
Hazelcast cluster starts, the first **C** members form the CP subsystem.
These members are called the CP members and they can also contain data for
the other regular Hazelcast data structures, such as `IMap`, `ISet`.

Data structures in the CP subsystem run in ``CPGroup``s. A _CP group_
consists of an odd number of ``CPMember``s between 3 and 7. Each CP
group independently runs the link:https://raft.github.io/[Raft consensus
algorithm^]. Operations are committed and executed only after they are
successfully replicated to the majority of the CP members in a CP group.
For instance, in a CP group of 5 CP members, operations are committed when
they are replicated to at least 3 CP members. The size of CP groups is
specified via `CPSubsystemConfig.setGroupSize(int)` and each CP group
contains the same number of CP members. See the <<cp-subsystem-configuration,
CP Subsystem Configuration section>> for configuration details.

Please note that the size of CP groups does not have to be
same with the CP member count. Namely, the number of CP members
in the CP subsystem can be larger than the configured CP group size.
In this case, CP groups are formed by selecting the CP members randomly.
Also note that the current CP subsystem implementation works only
in memory, without persisting any state to disk. It means that a
crashed CP member is not able to recover by reloading its previous
state. Therefore, crashed CP members create a danger for gradually
losing the majority of CP groups and eventually cause the total loss
of availability of the CP subsystem. To prevent such situations,
failed CP members can be removed from the CP subsystem and replaced
in CP groups with other available CP members. This flexibility provides
a good degree of fault tolerance at run-time. See the
<<cp-subsystem-management, CP Subsystem Management section>> for more details.

The CP subsystem runs 2 CP groups by default. The first one is the
_Metadata_ group. It is an internal CP group which is responsible
for managing the CP members and CP groups. It is initialized during
the cluster startup process if the CP subsystem is enabled via
`CPSubsystemConfig.setCPMemberCount(int)` configuration. The
second group is the _DEFAULT_ CP group, whose name is given in
`CPGroup.DEFAULT_GROUP_NAME`. If a group name is not specified while
creating a proxy for a CP data structure, that data structure is mapped
to the _DEFAULT_ CP group. For instance, when a CP `IAtomicLong` instance
is created by calling `CPSubsystem.getAtomicLong("myAtomicLong")`, it
will be initialized on the `DEFAULT` CP group. Besides these **2**
predefined CP groups, custom CP groups can be created at run-time.
If a CP `IAtomicLong` is created by calling
`CPSubsystem.getAtomicLong("myAtomicLong@myGroup")`, first a new
CP group is created with the name `myGroup` and then `myAtomicLong`
is initialized on this custom CP group.

The current set of CP data structures have quite low memory overheads.
Moreover, related to the Raft consensus algorithm, each CP group makes
use of internal heartbeat RPCs to maintain the authority of the leader
member and help lagging CP members to make progress. Last but not least,
the new CP Lock and Semaphore implementations rely on a brand new session
mechanism. In a nutshell, a Hazelcast member or client starts a new session
on the corresponding CP group when it makes its very first Lock or
Semaphore acquire request, and then periodically commits session heartbeats
to this CP group to indicate its liveliness. It means that if CP Locks and
Semaphores are distributed into multiple CP groups, there will be a session
management overhead. See the <<cp-sessions, CP Sessions section>> for more
details. For the aforementioned reasons, we recommend you to use a minimal
number of CP groups. For most use cases, the `DEFAULT` CP group should be
sufficient to maintain all CP data structure instances. Custom CP groups
could be created when the throughput of CP subsystem is needed to be improved.

**API Code Sample:**

[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=apisample]
----

[WARNING]
====
The CP data structure proxies differ from the other data structure
proxies in two aspects:

* Each time you fetch a proxy via one of the methods in this interface,
internally a commit is performed on the Metadata CP group. Hence, the
callers should cache  the returned proxies. 
* If you call the `DistributedObject.destroy()` method on a CP data
structure proxy, that data structure is terminated on the underlying
CP group and cannot be reinitialized until the CP group is force-destroyed.
For this reason, please make sure that you are completely done with a
CP data structure before destroying its proxy.
====

=== CP Subsystem Discovery

The CP subsystem runs a discovery process in the background on
cluster startup. When you enable it by setting a positive value
to `CPSubsystemConfig.setCPMemberCount(int)`, say `N`, the first
`N` members in the cluster member list initiate the discovery process.
Other Hazelcast members skip this step. The CP subsystem discovery process
runs out of the box on top of Hazelcast's cluster member list without
requiring any custom configuration for different environments. It is
completed when each one of the first `N` Hazelcast members initializes
its local CP member list and commits it to the Metadata CP group. The
Metadata CP group is initialized among those CP members as well.
*A soon-to-be CP member terminates itself if any of the following
conditions occur before the CP discovery process is completed:*

- Any Hazelcast member leaves the cluster,
- The local Hazelcast member commits a CP member list which is
different from other members' committed CP member lists,
- The local Hazelcast member list fails to commit its discovered
CP member list for any reason.

When the CP subsystem is restarted via
`CPSubsystemManagementService.restart()`, the CP subsystem discovery
process is triggered again. However, it does not terminate
Hazelcast members if the discovery fails for the aforementioned
reasons, because Hazelcast members are likely to contain data for
AP data structures and termination can cause data loss. Hence, you
need to observe the cluster and check if the discovery process completes
successfully on CP subsystem restart. See <<cp-subsystem-management-apis,
CP Subsystem Management APIs section>> for more details.

You can use the `CPSubsystemManagementService.awaitUntilDiscoveryCompleted(timeout, timeUnit)`
API to wait until the CP Subsystem discovery process is completed.

=== CP Sessions

For CP data structures which are performing ownership management of
the resources, such as Lock or Semaphore, a session is required to
keep track of the liveliness of the caller. In this context, the caller
means an entity that uses the CP subsystem APIs. It can be either a
Hazelcast member or a client. A caller initially creates a session
before sending its very first session based request to the CP group,
such as a Lock / Semaphore acquire. After creating a session on the CP
group, the caller stores its session ID locally and sends it alongside
its session based operations. A single session is used for all lock and
semaphore proxies of the caller. When a CP group receives a session based
operation, it checks the validity of the session using the session ID
information available in the operation. A session is valid if it is
still open in the CP group. An operation with a valid session ID is
accepted as a new session heartbeat. While a caller is idle, in other
words, it does not send any session based operation to the CP group for
a while, it commits periodic heartbeats to the CP group in the background
in order to keep its session alive. This interval is specified in
`CPSubsystemConfig.getSessionHeartbeatIntervalSeconds()`.

A session is closed when the caller does not touch the session during
a predefined duration. In this case, the caller is assumed to be
crashed and all its resources are released automatically. This duration
is specified in `CPSubsystemConfig.getSessionTimeToLiveSeconds()`.
See the <<cp-subsystem-configuration, CP Subsystem Configuration section>>
to learn the recommendations for choosing a reasonable session time-to-live duration.

Sessions offer a trade-off between liveliness and safety.
If you set a very small value using `CPSubsystemConfig.setSessionTimeToLiveSeconds(int)`,
then a session owner could be considered crashed very quickly and
its resources can be released prematurely. On the other hand, if you
set a large value, a session could be kept alive for an unnecessarily
long duration even if its owner actually crashes.

See the <<cp-subsystem-configuration, CP Subsystem Configuration section>>
for more details.

=== FencedLock

`FencedLock` is a linearizable & distributed & reentrant implementation
of `j.u.c.locks.Lock`. `FencedLock` is accessed via `CPSubsystem.getLock(String)`.
It is CP with respect to the CAP principle. It works on top of the
Raft consensus algorithm. It offers linearizability during crash-stop
failures and network partitions. If a network partition occurs, it
remains available on at most one side of the partition.
`FencedLock` works on top of CP sessions. Please see <<cp-sessions,
CP Sessions>> section for more information about CP sessions.

By default, `FencedLock` is reentrant. Once a caller acquires the lock,
it can acquire the lock reentrantly as many times as it wants in a
linearizable manner. You can configure the reentrancy behavior via
`FencedLockConfig`. For instance, reentrancy can be disabled and
`FencedLock` can work as a non-reentrant mutex. You can also set a
custom reentrancy limit. When the reentrancy limit is already reached,
`FencedLock` does not block a lock call. Instead, it fails with
`LockAcquireLimitReachedException` or a specified return value.
Please check the locking methods to see details about the behavior and
<<fencedlock-configuration, FencedLock Configuration section>> for the configuration.

Distributed locks are unfortunately *not equivalent* to single-node
mutexes because of the complexities in distributed systems, such as
uncertain communication patterns, and independent and partial failures.
In an asynchronous network, no lock service can guarantee mutual exclusion,
because there is no way to distinguish between a slow and a crashed process.
Consider the following scenario, where a Hazelcast client acquires a
`FencedLock`, then hits a long GC pause. Since it will not be able to
commit session heartbeats while paused, its CP session will be eventually
closed. After this moment, another Hazelcast client can acquire this lock.
If the first client wakes up again, it may not immediately notice that it
has lost ownership of the lock. In this case, multiple clients think they
hold the lock. If they attempt to perform an operation on a shared resource,
they can break the system. To prevent such situations, you can choose to
use an infinite session timeout, but this time probably you are going to
deal with liveliness issues. For the scenario above, even if the first
client actually crashes, the requests sent by two clients can be reordered
in the network and hit the external resource in the reverse order.

There is a simple solution for this problem. Lock holders are ordered by
a monotonic fencing token, which increments each time the lock is assigned
to a new owner. This fencing token can be passed to external services or
resources to ensure sequential execution of the side effects performed by lock holders.

The following diagram illustrates the idea. Client-1 acquires the lock
first and receives `1` as its fencing token. Then, it passes this token
to the external service, which is our shared resource in this scenario.
Just after that, Client-1 hits a long GC pause and eventually loses
ownership of the lock because it misses to commit CP session heartbeats.
Then, Client-2 chimes in and acquires the lock. Similar to Client-1,
Client-2 passes its fencing token to the external service. After that,
once Client-1 comes back alive, its write request will be rejected by
the external service, and only Client-2 will be able to safely talk to it.

image::FencedLock.png[Fenced Lock]

You can read more about the fencing token idea in Martin Kleppmann's
link:https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html[How to do distributed locking^]
blog post and Google's link:https://ai.google/research/pubs/pub27897[Chubby paper^].
`FencedLock` integrates this idea with the `j.u.c.locks.Lock` abstraction, excluding
`j.u.c.locks.Condition`. `newCondition()` is not implemented and throws
`UnsupportedOperationException`.

All of the API methods in the new `FencedLock` abstraction offer
exactly-once execution semantics. For instance, even if a `lock()`
call is internally retried because of a crashed CP member, the lock
is acquired only once. The same rule also applies to the other methods
in the API.

=== Configuration

==== CP Subsystem Configuration

* `cp-member-count`: Number of ``CPMember``s to initialize the
`CPSubsystem`. It is `0` by default, meaning that the CP subsystem
is disabled. The CP subsystem is enabled when a positive value
is set. After the CP subsystem is initialized successfully, more
CP members can be added at run-time and the number of active CP
members can go beyond the configured CP member count. The number
of CP members can be smaller than the total size of the Hazelcast
cluster. For instance, you can run 5 CP members in a 20-member
Hazelcast cluster.
+
If set, must be greater than or equal to `group-size`.
+
* `group-size`: Number of CP members to run CP groups. If set,
it must be an odd number between `3` and `7`. Otherwise,
`cp-member-count` is respected.
+
If set, must be smaller than or equal to `cpMemberCount`.
+
* `session-time-to-live-seconds`: Duration for a CP session to
be kept alive after the last received heartbeat. The session will
be closed if there is no new heartbeat during this duration.
Session TTL must be decided wisely. If a very low value is set,
CP session of a Hazelcast instance can be closed prematurely if
the instance temporarily loses connectivity to the CP subsystem
because of a network partition or a GC pause. In such an occasion,
all CP resources of this Hazelcast instance, such as `FencedLock`
or `ISemaphore`, are released. On the other hand, if a very large
value is set, CP resources can remain assigned to an actually crashed
Hazelcast instance for too long and liveliness problems can occur. The
CP subsystem offers an API `CPSessionManagementService`, to deal with
liveliness issues related to CP sessions. In order to prevent premature
session expires, session TTL configuration can be set a relatively large
value and `CPSessionManagementService.forceCloseSession(String, long)`
can be manually called to close CP session of a crashed Hazelcast instance.
+
Must be greater than `session-heartbeat-interval-seconds`, and smaller
than or equal to `missing-cp-member-auto-removal-seconds`. Default
value is `300` seconds.
+
* `session-heartbeat-interval-seconds`: Interval for the
periodically-committed CP session heartbeats. A CP session is
started on a CP group with the first session based request of a
Hazelcast instance. After that moment, heartbeats are periodically
committed to the CP group.
+
Must be smaller than `session-time-to-live-seconds`.
Default value is `5` seconds.
+
* `missing-cp-member-auto-removal-seconds`: Duration to wait
before automatically removing a missing CP member from the CP subsystem.
When a CP member leaves the cluster, it is not automatically removed
from the CP subsystem, since it could be still alive and left the
cluster because of a network partition. On the other hand, if a missing
CP member is actually crashed, it creates a danger for its CP groups,
because it will be still part of majority calculations. This situation
could lead to losing majority of CP groups if multiple CP members leave
the cluster over time.
+
With the default configuration, missing CP members will be automatically
removed from the CP subsystem after `4` hours. This feature is very
useful in terms of fault tolerance when CP member count is also configured
to be larger than group size. In this case, a missing CP member will be
safely replaced in its CP groups with other available CP members in the
CP subsystem. This configuration also implies that no network partition
is expected to be longer than the configured duration.
+
If a missing CP member comes back alive after it is automatically
removed from the CP subsystem with this feature, that CP member must
be terminated manually.
+
Must be greater than or equal to `session-time-to-live-seconds`.
Default value is `14400` seconds (4 hours).
+
* `fail-on-indeterminate-operation-state`: Offers a choice between
at-least-once and at-most-once execution of the operations on top of
the Raft consensus algorithm. It is disabled by default and offers
at-least-once execution guarantee. If enabled, it switches to at-most-once
execution guarantee. When you invoke an API method on a CP data structure
proxy, it replicates an internal operation to the corresponding CP group.
After this operation is committed to majority of this CP group by the
Raft leader node, it sends a response for the public API call.
If a failure causes loss of the response, then the calling side cannot
determine if the operation is committed on the CP group or not.
In this case, if this configuration is disabled, the operation is replicated
again to the CP group, and hence could be committed multiple times.
If it is enabled, the public API call fails with `IndeterminateOperationStateException`.
+
Default value is `false`.

**Declarative Configuration:**

[source,xml]
----
<hazelcast>
    ...
    <cp-subsystem>
        <cp-member-count>7</cp-member-count>
        <group-size>3</group-size>
        <session-time-to-live-seconds>300</session-time-to-live-seconds>
        <session-heartbeat-interval-seconds>5</session-heartbeat-interval-seconds>
        <missing-cp-member-auto-removal-seconds>14400</missing-cp-member-auto-removal-seconds>
        <fail-on-indeterminate-operation-state>false</fail-on-indeterminate-operation-state>
    </cp-subsystem>
    ...
</hazelcast>
----

**Programmatic Configuration:**

[source,java]
----
include::{javasource}/cp/CpSubsystemConfiguration.java[tag=cpconf]
----

==== FencedLock Configuration

* `name`: Name of the `FencedLock`.
* `lock-acquire-limit`: Maximum number of reentrant lock acquires.
Once a caller acquires the lock this many times, it will not be
able to acquire the lock again, until it makes at least one `unlock()` call.
+
By default, no upper bound is set for the number of reentrant
lock acquires, which means that once a caller acquires a
`FencedLock`, all of its further `lock()` calls will succeed.
However, for instance, if you set `lock-acquire-limit` to `2`,
once a caller acquires the lock, it will be able to acquire it
once more, but its third `lock()` call will not succeed.
+
If `lock-acquire-limit` is set to 1, then the lock becomes non-reentrant.
+
`0` means there is no upper bound for the number of reentrant
lock acquires. Default value is `0`.

**Declarative Configuration:**

[source,xml]
----
<hazelcast>
    ...
    <cp-subsystem>
        ...
        <locks>
            <fenced-lock>
                <name>reentrant-lock</name>
                <lock-acquire-limit>0</lock-acquire-limit>
            </fenced-lock>
            <fenced-lock>
                <name>limited-reentrant-lock</name>
                <lock-acquire-limit>10</lock-acquire-limit>
            </fenced-lock>
            <fenced-lock>
                <name>non-reentrant-lock</name>
                <lock-acquire-limit>1</lock-acquire-limit>
            </fenced-lock>
        </locks>
    </cp-subsystem>
    ...
</hazelcast>
----

**Programmatic Configuration:**

[source,java]
----
include::{javasource}/cp/CpSubsystemConfiguration.java[tag=cplockconf]
----

==== Semaphore Configuration

* `name`: Name of the CP `ISemaphore`.
* `jdk-compatible`: Enables / disables JDK compatibility
of the CP `ISemaphore`. When it is JDK compatible, just as
in the `j.u.c.Semaphore.release()` method, a permit can be
released without acquiring it first, because acquired permits
are not bound to threads. However, there is no auto-cleanup of
the acquired permits upon Hazelcast server / client failures.
If a permit holder fails, its permits must be released manually.
When JDK compatibility is disabled, a `HazelcastInstance` must
acquire permits before releasing them and it cannot release a permit
that it has not acquired. It means, you can acquire a permit from
one thread and release it from another thread using the same
`HazelcastInstance`, but not different `HazelcastInstance`s.
In this mode, acquired permits are automatically released upon
failure of the holder `HazelcastInstance`. So there is a minor
behavioral difference to the `j.u.c.Semaphore.release()` method.
+
JDK compatibility is disabled by default.

**Declarative Configuration:**

[source,xml]
----
<hazelcast>
    ...
    <cp-subsystem>
        ...
        <semaphores>
            <cp-semaphore>
                <name>jdk-compatible-semaphore</name>
                <jdk-compatible>true</jdk-compatible>
            </cp-semaphore>
            <cp-semaphore>
                <name>another-semaphore</name>
                <jdk-compatible>false</jdk-compatible>
            </cp-semaphore>
        </semaphores>
    </cp-subsystem>
    ...
</hazelcast>
----

**Programmatic Configuration:**

[source,java]
----
include::{javasource}/cp/CpSubsystemConfiguration.java[tag=cpsemaconf]
----

==== Raft Algorithm Configuration

WARNING: These parameters tune specific parameters of
Hazelcast's Raft consensus algorithm implementation and are only for power users.

* `leader-election-timeout-in-millis`: Leader election timeout
in milliseconds. If a candidate cannot win the majority of votes
in time, a new election round is initiated. Default value is `2000` milliseconds.
* `leader-heartbeat-period-in-millis`: Period in milliseconds
for a leader to send heartbeat messages to its followers. 
efault value is `5000` milliseconds.
* `max-missed-leader-heartbeat-count`: Maximum number of missed
leader heartbeats to trigger a new leader election. Default value is `5`.
* `append-request-max-entry-count`: Maximum entry count that can
be sent in a single batch of append entries request. Default value is `100`.
* `commit-index-advance-count-to-snapshot`: Number of new commits
to initiate a new snapshot after the last snapshot. Default value is `10000`.
* `uncommitted-entry-count-to-reject-new-appends`: Maximum number
of uncommitted entries in the leader's Raft log before temporarily
rejecting the new requests of callers. Default value is `100`.
* `append-request-backoff-timeout-in-millis`: Timeout in milliseconds
for append request backoff. After the leader sends an append request
to a follower, it will not send a subsequent append request until the
follower responds to the former request or this timeout occurs. Default
value is `100` milliseconds.

**Declarative Configuration:**

[source,xml]
----
<hazelcast>
    ...
    <cp-subsystem>
        ...
        <raft-algorithm>
            <leader-election-timeout-in-millis>2000</leader-election-timeout-in-millis>
            <leader-heartbeat-period-in-millis>5000</leader-heartbeat-period-in-millis>
            <max-missed-leader-heartbeat-count>5</max-missed-leader-heartbeat-count>
            <append-request-max-entry-count>100</append-request-max-entry-count>
            <commit-index-advance-count-to-snapshot>10000</commit-index-advance-count-to-snapshot>
            <uncommitted-entry-count-to-reject-new-appends>200</uncommitted-entry-count-to-reject-new-appends>
            <append-request-backoff-timeout-in-millis>250</append-request-backoff-timeout-in-millis>
        </raft-algorithm>
        ...
    </cp-subsystem>
    ...
</hazelcast>
----

**Programmatic Configuration:**

[source,java]
----
include::{javasource}/cp/CpSubsystemConfiguration.java[tag=cpraftconf]
----


=== CP Subsystem Management

Unlike the dynamic nature of Hazelcast clusters, the
CP subsystem requires manual intervention while expanding/shrinking
its size, or when a CP member crashes or becomes unreachable. When a
CP member becomes unreachable, it cannot be automatically removed from
the CP subsystem because it could be still alive and partitioned away.

Moreover, the current CP subsystem implementation works only in memory
without persisting any state to disk. It means that a crashed CP member
will not be able to recover by reloading its previous state. Therefore,
crashed CP members create a danger for gradually losing the majority of
CP groups and eventually total loss of the availability of the CP
subsystem. To prevent such situations, `CPSubsystemManagementService`
offers APIs for dynamic management of the CP members.

The CP subsystem relies on Hazelcast's failure detectors to test the
reachability of CP members. Before removing a CP member from the CP
subsystem, please make sure that it is declared as unreachable by
Hazelcast's failure detector and removed from the Hazelcast's member list.

CP member additions and removals are internally handled by performing
a single membership change at a time. When multiple CP members are
shutting down concurrently, their shutdown process is executed serially.
First, the Metadata CP group creates a membership change plan for CP groups.
Then, the scheduled changes are applied to the CP groups one by one.
After all removals are done, the shutting down CP member is removed
from the active CP members list and its shutdown process is completed.

When a CP member is being shut down, it is replaced with another
available CP member in all of its CP groups, including the Metadata
group, in order not to decrease or more importantly not to lose the
majority of CP groups. If there is no available CP member to replace
a shutting down CP member in a CP group, that group's size is reduced
by 1 and its majority value is recalculated.

A new CP member can be added to the CP subsystem to either increase
the number of available CP members for new CP groups or to fulfill
the missing slots in the existing CP groups. After the initial
Hazelcast cluster startup is done, an existing Hazelcast member
can be be promoted to the CP member role. This new CP member
automatically joins to CP groups that have missing members, and
the majority value of these CP groups is recalculated.

A CP member may crash due to hardware problems or a defect in user
code, or it may become unreachable because of connection problems,
such as network partitions, network hardware failures, etc. If a
CP member is known to be alive but only has temporary communication
issues, it will catch up the other CP members and continue to operate
normally after its communication issues are resolved. If it is known
to be crashed or communication issues cannot be resolved in a short
time, it can be preferable to remove this CP member from the CP
subsystem, hence from all its CP groups. In this case, the unreachable
CP member should be terminated to prevent any accidental communication
with the rest of the CP subsystem.

When the majority of a CP group is lost for any reason, that CP group
cannot make progress anymore. Even a new CP member cannot join to this
CP group, because membership changes also go through the Raft consensus
algorithm. For this reason, the only option is to force-destroy the CP
group via the `CPSubsystemManagementService.forceDestroyCPGroup()` API.
When this API is used, the CP group is terminated non-gracefully,
without the Raft algorithm mechanics. Then, all CP data structure
proxies that talk to this CP group fail with `CPGroupDestroyedException`.
However, if a new proxy is created afterwards, then this CP group will be
recreated from the scratch with a new set of CP members. Losing the majority
of a CP group can be likened to partition-loss scenario of AP Hazelcast.

Please note that the CP groups that have lost their majority must be
force-destroyed immediately, because they can block the Metadata CP
group to perform membership changes.

Loss of the majority of Metadata CP group is the doomsday scenario for
the CP subsystem. It is a fatal failure and the only solution is to
reset the whole CP subsystem state via the `CPSubsystemManagementService.restart()` API.
To be able to reset the CP subsystem, the initial size of the
CP subsystem must be satisfied, which is defined by `CPSubsystemConfig.getCPMemberCount()`.
For instance, assuming that `CPSubsystemConfig.getCPMemberCount()`
is 5 and only 1 CP member is currently alive, when
`CPSubsystemManagementService.restart()` is called, additional 4 regular
Hazelcast members should exist in the cluster. New Hazelcast members
can be started to satisfy `CPSubsystemConfig.getCPMemberCount()`.

NOTE: There is a subtle point about graceful shutdown of CP members.
If there are `N` CP members in the cluster, `HazelcastInstance.shutdown()`
can be called on `N-2` CP members concurrently. Once these `N-2` CP
members complete their shutdown, the remaining `2` CP members must be
shut down serially. Even though the shutdown API is called concurrently
on multiple members, the Metadata CP group handles shutdown requests
serially. Therefore, it would be simpler to shut down CP members one by one,
by calling `HazelcastInstance.shutdown()` on the next CP member once the
current CP member completes its shutdown. The reason behind this limitation
is, each shutdown request internally requires a Raft commit to the Metadata
CP group. A CP member proceeds to shutdown after it receives a response
of its commit to the Metadata CP group. To be able to perform a Raft commit,
the Metadata CP group must have its majority available. When there are only
`2` CP members left after graceful shutdowns, the majority of the Metadata
CP group becomes `2`. If the last `2` CP members shut down concurrently,
one of them is likely to perform its Raft commit faster than the other one
and leave the cluster before the other CP member completes its Raft commit.
In this case, the last CP member waits for a response of its commit attempt
on the Metadata group, and times out eventually. This situation causes an
unnecessary delay on shutdown process of the last CP member. On the other hand,
when the last `2` CP members shut down serially, the `N-1`th member receives
response of its commit after its shutdown request is committed also on the last
CP member. Then, the last CP member checks its local data to notice that it is
the last CP member alive, and proceeds its shutdown without attempting a Raft
commit on the Metadata CP group.

==== CP Subsystem Management APIs

You can access the CP subsystem management APIs using the Java API or
REST interface. To communicate with the REST interface there are two
options; one is to access REST endpoint URL directly or using the
`cp-subsystem.sh` shell script, which comes with the Hazelcast package.

NOTE: The `cp-cluster.sh` script uses `curl` command, and `curl`
must be installed to be able to use the script.

* **Get Local CP Member:**
+
Returns the local CP member if this Hazelcast member is a part of the CP Subsystem.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=localmember]
----
+
.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/members/local
OR
> sh cp-subsystem.sh -o get-local-member --address 127.0.0.1 --port 5701
+
Sample Response:
{
    "uuid": "6428d7fd-6079-48b2-902c-bdf6a376051e",
    "address": "[127.0.0.1]:5701"
}
----
+
* **Get CP Groups:**
+
Returns the list of active CP groups.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=cpgroups]
----
+
.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups
OR
> sh cp-subsystem.sh -o get-groups --address 127.0.0.1 --port 5701
+
Sample Response:
[{
    "name": "METADATA",
    "id": 0
}, {
    "name": "atomics",
    "id": 8
}, {
    "name": "locks",
    "id": 14
}]
----
+
* **Get a single CP Group:**
+
Returns the active CP group with the given name. There can be at
most one active CP group with a given name.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=cpgroup]
----
+
.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups/${CPGROUP_NAME}
OR
> sh cp-subsystem.sh -o get-group --group ${CPGROUP_NAME} --address 127.0.0.1 --port 5701
+
Sample Response:
{
    "id": {
        "name": "locks",
        "id": 14
    },
    "status": "ACTIVE",
    "members": [{
        "uuid": "33f84b0f-46ba-4a41-9e0a-29ee284c1c2a",
        "address": "[127.0.0.1]:5703"
    }, {
        "uuid": "59ca804c-312c-4cd6-95ff-906b2db13acb",
        "address": "[127.0.0.1]:5704"
    }, {
        "uuid": "777ff6ea-b8a3-478d-9642-47d1db019b37",
        "address": "[127.0.0.1]:5705"
    }, {
        "uuid": "c7856e0f-25d2-4717-9919-88fb3ecb3384",
        "address": "[127.0.0.1]:5702"
    }, {
        "uuid": "c6229b44-8976-4602-bb57-d13cf743ccef",
        "address": "[127.0.0.1]:5701"
    }]
}
----
+
* **Get CP Members:**
+
Returns the list of active CP members in the cluster.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=cpmembers]
----
+
.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/members
OR
> sh cp-subsystem.sh -o get-members --address 127.0.0.1 --port 5701
+
Sample Response:
[{
    "uuid": "33f84b0f-46ba-4a41-9e0a-29ee284c1c2a",
    "address": "[127.0.0.1]:5703"
}, {
    "uuid": "59ca804c-312c-4cd6-95ff-906b2db13acb",
    "address": "[127.0.0.1]:5704"
}, {
    "uuid": "777ff6ea-b8a3-478d-9642-47d1db019b37",
    "address": "[127.0.0.1]:5705"
}, {
    "uuid": "c6229b44-8976-4602-bb57-d13cf743ccef",
    "address": "[127.0.0.1]:5701"
}, {
    "uuid": "c7856e0f-25d2-4717-9919-88fb3ecb3384",
    "address": "[127.0.0.1]:5702"
}]
----
+
* **Force Destroy a CP Group:**
+
Unconditionally destroys the given active CP group without using the
Raft algorithm mechanics. This method must be used only when a
CP group loses its majority and cannot make progress anymore.
Normally, membership changes in CP groups, such as CP member
promotion or removal, are done via the Raft consensus algorithm.
However, when a CP group loses its majority, it will not be able to
commit any new operation. Therefore, this method ungracefully
terminates the remaining members of the given CP group. It also performs
a Raft commit to the Metadata CP group in order    to update the status
of the destroyed group. Once a CP group ID is destroyed, all CP data
structure proxies created before the destroy fails with `CPGroupDestroyedException`.
+
Once a CP group is destroyed, it can be created again with a new
set of CP members. This method is idempotent. It has no effect if
the given CP group is already destroyed.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=destroygroup]
----
+
.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups/${CPGROUP_NAME}/remove
OR
> sh cp-subsystem.sh -o force-destroy-group --group ${CPGROUP_NAME} --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----
+
* **Remove a CP Member:**
+
Removes the given unreachable CP member from the active CP members
list and all CP groups it belongs to. If any other active CP member
is available, it will replace the removed CP member in its CP groups.
Otherwise, CP groups which the removed CP member is a member of will
shrink and their majority values will be recalculated.
+
WARNING: Before removing a CP member from the CP subsystem, please
make sure that it is declared as unreachable by Hazelcast's failure
detector and removed from Hazelcast's member list. The behavior is
undefined when a running CP member is removed from the CP subsystem.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=removemember]
----
+
.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/members/${CPMEMBER_UUID}/remove
OR
> sh cp-subsystem.sh -o remove-member --member ${CPMEMBER_UUID} --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----
+
* **Promote Local Member to a CP Member**
+
Promotes the local Hazelcast member to a CP member. If the local
member is already in the active CP members list, then this method
has no effect. When the current member is promoted to a CP member,
its member UUID is assigned as CP member UUID. The promoted CP member
will be added to the CP groups that have missing members, i.e.,
whose size is smaller than `CPSubsystemConfig.getGroupSize()`.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=promotemember]
----
+
.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/members
OR
> sh cp-subsystem.sh -o promote-member --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----
+
* **Wipe and Restart CP Subsystem**
+
Wipes and resets the whole CP subsystem and initializes it as if
the Hazelcast cluster is starting up initially. This method must
be used only when the Metadata CP group loses its majority and
cannot make progress anymore.
+
After this method is called, all CP state and data are wiped and
the CP members start with empty state.
+
This method can be invoked only from the oldest member in the
Hazelcast cluster. Moreover, the Hazelcast cluster must have
at least `CPSubsystemConfig.getCPMemberCount()` members.
+
This method must not be called while there are membership changes
in the cluster. Before calling this method, please make sure that
there is no new member joining and all existing Hazelcast members
have seen the same member list.
+
WARNING: This method is **NOT** idempotent and multiple invocations
can break the whole system! After calling this API, you must observe
the system to see if the restart process is successfully completed or
failed before making another call.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=restart]
----
+
.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/restart
OR
> sh cp-subsystem.sh -o restart --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----

==== Session Management API

There are two management API methods for session management.

* **Get CP Group Sessions:**
+
Returns all CP sessions that are currently active in a CP group.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=sessions]
----
+
.REST API
[source,sh]
----
> curl http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups/${CPGROUP_NAME}/sessions
OR
> sh cp-subsystem.sh -o get-sessions --group ${CPGROUP_NAME} --address 127.0.0.1 --port 5701
+
Sample Response:
[{
    "id": 1,
    "creationTime": 1549008095530,
    "expirationTime": 1549008766630,
    "version": 73,
    "endpoint": "[127.0.0.1]:5701",
    "endpointType": "SERVER",
    "endpointName": "hz-member-1"
}, {
    "id": 2,
    "creationTime": 1549008115419,
    "expirationTime": 1549008765425,
    "version": 71,
    "endpoint": "[127.0.0.1]:5702",
    "endpointType": "SERVER",
    "endpointName": "hz-member-2"
}]
----
+
* **Force Close a Session:**
+
If a Hazelcast instance that owns a CP session crashes, its CP session
is not terminated immediately. Instead, the session is closed after
`CPSubsystemConfig.getSessionTimeToLiveSeconds()` passes. If it is
known for sure that the session owner is not partitioned and definitely
crashed, this method can be used for closing the session and releasing
its resources immediately.
+
.Java API
[source,java]
----
include::{javasource}/cp/CpSubsystemAPI.java[tag=closesession]
----
+
.REST API
[source,sh]
----
> curl -X POST --data "${GROUPNAME}&${PASSWORD}" http://127.0.0.1:5701/hazelcast/rest/cp-subsystem/groups/${CPGROUP_NAME}/sessions/${CP_SESSION_ID}/remove
OR
> sh cp-subsystem.sh -o force-close-session --group ${CPGROUP_NAME} --session-id ${CP_SESSION_ID} --address 127.0.0.1 --port 5701 --groupname ${GROUPNAME} --password ${PASSWORD}
----
